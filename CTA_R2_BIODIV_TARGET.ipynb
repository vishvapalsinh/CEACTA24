{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTA BIODIV ROUND 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "from ratelimit import limits, sleep_and_retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All path can be set according to your local repo\n",
    "# Folder path containing ground truths\n",
    "folder_path_ground_truth = \"Dataset/tbiodiv10-0.01-sample/horizontal/gt/\"\n",
    "\n",
    "# Folder path given the target \n",
    "folder_path_tragets = \"Dataset/tbiodiv10-0.01-sample/horizontal/targets/\"\n",
    "\n",
    "# Define folder path containing tables\n",
    "folder_path_tables = \"Dataset/tbiodiv10-0.01-sample/horizontal/tables/\"\n",
    "\n",
    "file_name_cta_gt = \"cta_gt.csv\"\n",
    "file_name_cta_target = \"cta_targets.csv\"\n",
    "file_name_specific_table = \"AOD06100110I140.csv\" # AZC06100207I0103, WIT06100910I067\n",
    "\n",
    "file_path_cta_target = folder_path_tragets + file_name_cta_target\n",
    "file_path_cta_gt = folder_path_ground_truth + file_name_cta_gt\n",
    "file_path_specific_table = folder_path_tables + file_name_specific_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the Target csv for the CTA task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the target \n",
    "table_biodiv_cta_target = pd.read_csv(file_path_cta_target,header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the target dataframe is: (5250, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EGN060702I0010</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EGN060702I0010</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EGN060702I0031</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EGN060702I0072</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EGN060702I0114</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0  1\n",
       "0  EGN060702I0010  1\n",
       "1  EGN060702I0010  3\n",
       "2  EGN060702I0031  1\n",
       "3  EGN060702I0072  2\n",
       "4  EGN060702I0114  2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Shape of the target dataframe is: {table_biodiv_cta_target.shape}\")\n",
    "table_biodiv_cta_target.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique table in given in the target CTA file 1317\n",
      "Total duplicate row in the given target CTA files are: 0\n"
     ]
    }
   ],
   "source": [
    "# Check total unique tables in it having the first index containing table names.\n",
    "print(f\"Total unique table in given in the target CTA file {table_biodiv_cta_target[0].nunique()}\")\n",
    "\n",
    "# Check the duplicate rows in the csv file\n",
    "duplicates = table_biodiv_cta_target.duplicated(subset=[table_biodiv_cta_target.columns[0], table_biodiv_cta_target.columns[1]])\n",
    "print(f\"Total duplicate row in the given target CTA files are: {duplicates.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the Ground truth csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_biodiv_cta_gt = pd.read_csv(file_path_cta_gt, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the ground truth dataset is : (5250, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EGN060702I0010</td>\n",
       "      <td>1</td>\n",
       "      <td>http://www.wikidata.org/entity/Q484170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EGN060702I0010</td>\n",
       "      <td>3</td>\n",
       "      <td>http://www.wikidata.org/entity/Q5162954,http:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EGN060702I0031</td>\n",
       "      <td>1</td>\n",
       "      <td>NIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EGN060702I0072</td>\n",
       "      <td>2</td>\n",
       "      <td>http://www.wikidata.org/entity/Q16695773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EGN060702I0114</td>\n",
       "      <td>2</td>\n",
       "      <td>NIL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0  1                                                  2\n",
       "0  EGN060702I0010  1             http://www.wikidata.org/entity/Q484170\n",
       "1  EGN060702I0010  3  http://www.wikidata.org/entity/Q5162954,http:/...\n",
       "2  EGN060702I0031  1                                                NIL\n",
       "3  EGN060702I0072  2           http://www.wikidata.org/entity/Q16695773\n",
       "4  EGN060702I0114  2                                                NIL"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Shape of the ground truth dataset is : {table_biodiv_cta_gt.shape}\")\n",
    "table_biodiv_cta_gt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique table in given in the ground truth CTA file 1317\n",
      "Total duplicate row in the given ground truth CTA files are: 0\n"
     ]
    }
   ],
   "source": [
    "# Check total unique tables in it having the first index containing table names.\n",
    "print(f\"Total unique table in given in the ground truth CTA file {table_biodiv_cta_gt[0].nunique()}\")\n",
    "\n",
    "# Check the duplicate rows in the csv file\n",
    "duplicates = table_biodiv_cta_gt.duplicated(subset=[table_biodiv_cta_gt.columns[0], table_biodiv_cta_gt.columns[1]])\n",
    "print(f\"Total duplicate row in the given ground truth CTA files are: {duplicates.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of the merged dataframe is : (5250, 2)\n"
     ]
    }
   ],
   "source": [
    "### Finding common rows between target and ground samples \n",
    "## Get the first three column from both the table \n",
    "table_biodiv_cta_target_subset = table_biodiv_cta_target.iloc[:, :2]\n",
    "table_biodiv_cta_gt_subset = table_biodiv_cta_gt.iloc[:, :2]\n",
    "\n",
    "# Check whether all the rows are same or not\n",
    "merged_df = pd.merge(table_biodiv_cta_target_subset, table_biodiv_cta_gt_subset, on=[0,1], how = 'inner')\n",
    "print(f\"The dimension of the merged dataframe is : {merged_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the column_values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A faster way to fetch the specific column from the table and then store its values in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read CSV files into a dictionary\n",
    "def load_tables(folder_path_tables):\n",
    "    tables = {}\n",
    "    for filename in os.listdir(folder_path_tables):\n",
    "        if filename.endswith('.csv'):\n",
    "            table_name = filename[:-4]  # Remove '.csv' extension\n",
    "            table_path = os.path.join(folder_path_tables, filename)\n",
    "            tables[table_name] = pd.read_csv(table_path)\n",
    "    return tables\n",
    "\n",
    "def get_value_from_preloaded_tables(row, tables):\n",
    "    table_name = row[0]\n",
    "    column_number = row[1]\n",
    "\n",
    "    if table_name in tables:\n",
    "        df_table = tables[table_name]\n",
    "        if column_number < len(df_table.columns):\n",
    "            values = df_table.iloc[:, column_number]\n",
    "            # Join the cell values with ' || ' as the separator\n",
    "            return ' || '.join(values.astype(str).tolist())\n",
    "    return None\n",
    "\n",
    "# Function to fetch values in parallel\n",
    "def fetch_values_in_parallel(df, tables):\n",
    "    rows = df.itertuples(index=False)\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        fetched_values = list(executor.map(lambda row: get_value_from_preloaded_tables(row, tables), rows))\n",
    "    return fetched_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.4 s, sys: 3.62 s, total: 27 s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load all tables into a dictionary\n",
    "tables = load_tables(folder_path_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.29 s, sys: 637 ms, total: 2.93 s\n",
      "Wall time: 2.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fetch values in parallel\n",
    "fetched_values = fetch_values_in_parallel(table_biodiv_cta_target, tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the fetched values as a new column in the dataframe\n",
    "table_biodiv_cta_target['column_values'] = fetched_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>column_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EGN060702I0010</td>\n",
       "      <td>1</td>\n",
       "      <td>Marchamp || Saint-Maurice-de-Gourdans || Nivig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EGN060702I0010</td>\n",
       "      <td>3</td>\n",
       "      <td>zone naturelle d'intérêt écologique, faunistiq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EGN060702I0031</td>\n",
       "      <td>1</td>\n",
       "      <td>Category:Judiciary of Iran || Category:Judicia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EGN060702I0072</td>\n",
       "      <td>2</td>\n",
       "      <td>Wikipedia:Vital articles/Level/4 || Wikipedia:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EGN060702I0114</td>\n",
       "      <td>2</td>\n",
       "      <td>Category:February 1910 events || Category:Febr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0  1                                      column_values\n",
       "0  EGN060702I0010  1  Marchamp || Saint-Maurice-de-Gourdans || Nivig...\n",
       "1  EGN060702I0010  3  zone naturelle d'intérêt écologique, faunistiq...\n",
       "2  EGN060702I0031  1  Category:Judiciary of Iran || Category:Judicia...\n",
       "3  EGN060702I0072  2  Wikipedia:Vital articles/Level/4 || Wikipedia:...\n",
       "4  EGN060702I0114  2  Category:February 1910 events || Category:Febr..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_biodiv_cta_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_values(cell_value):\n",
    "    # Split the cell value by '||'\n",
    "    groups = cell_value.split('||')\n",
    "    cleaned_groups = []\n",
    "\n",
    "    for group in groups:\n",
    "        # Split each group by ',' and remove duplicates while preserving order\n",
    "        elements = group.split(',')\n",
    "        # Remove 'nan' values and strip whitespace\n",
    "        elements = [element.strip() for element in elements if element.strip().lower() != 'nan']\n",
    "        if elements:  # Only add non-empty groups\n",
    "            unique_elements = ', '.join(OrderedDict.fromkeys(elements))\n",
    "            cleaned_groups.append(unique_elements)\n",
    "    \n",
    "    # Remove duplicate groups while preserving order\n",
    "    cleaned_groups = list(OrderedDict.fromkeys(cleaned_groups))\n",
    "\n",
    "    # Join the cleaned groups back with '||'\n",
    "    cleaned_value = ' || '.join(cleaned_groups)\n",
    "    return cleaned_value\n",
    "\n",
    "def clean_and_split_text(cell_value):\n",
    "    # Split by '||' first\n",
    "    groups = cell_value.split('||')\n",
    "    cleaned_groups = []\n",
    "\n",
    "    for group in groups:\n",
    "        if group.strip():\n",
    "            # Split each group by ',' followed by a space and a capital letter\n",
    "            segments = re.split(r',\\s+(?=[A-Z])', group)\n",
    "            cleaned_segments = []\n",
    "            \n",
    "            for segment in segments:\n",
    "                # Remove duplicates within the segment split by ','\n",
    "                elements = segment.split(',')\n",
    "                # Remove 'nan' values and strip whitespace\n",
    "                elements = [element.strip() for element in elements if element.strip().lower() != 'nan']\n",
    "                if elements:  # Only add non-empty segments\n",
    "                    unique_elements = ', '.join(OrderedDict.fromkeys(elements))\n",
    "                    cleaned_segments.append(unique_elements)\n",
    "            \n",
    "            # Join cleaned segments and maintain their order\n",
    "            if cleaned_segments:\n",
    "                cleaned_group = ' || '.join(cleaned_segments)\n",
    "                cleaned_groups.append(cleaned_group)\n",
    "        else:\n",
    "            cleaned_groups.append(group.strip())\n",
    "    \n",
    "    # Join the cleaned groups back with '||'\n",
    "    cleaned_value = ' || '.join(cleaned_groups)\n",
    "    \n",
    "    # Remove leading and trailing '||' if present\n",
    "    cleaned_value = cleaned_value.strip(' ||')\n",
    "    \n",
    "    return cleaned_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_cleaning_function(cell_value):\n",
    "    # First clean using clean_column_values\n",
    "    cleaned_value = clean_column_values(cell_value)\n",
    "    # Then further clean using clean_and_split_text\n",
    "    fully_cleaned_value = clean_and_split_text(cleaned_value)\n",
    "    return fully_cleaned_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_biodiv_cta_target['clean_column_values'] = table_biodiv_cta_target.iloc[:, 2].apply(combined_cleaning_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>column_values</th>\n",
       "      <th>clean_column_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EGN060702I0010</td>\n",
       "      <td>1</td>\n",
       "      <td>Marchamp || Saint-Maurice-de-Gourdans || Nivig...</td>\n",
       "      <td>Marchamp || Saint-Maurice-de-Gourdans || Nivig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EGN060702I0010</td>\n",
       "      <td>3</td>\n",
       "      <td>zone naturelle d'intérêt écologique, faunistiq...</td>\n",
       "      <td>zone naturelle d'intérêt écologique, faunistiq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EGN060702I0031</td>\n",
       "      <td>1</td>\n",
       "      <td>Category:Judiciary of Iran || Category:Judicia...</td>\n",
       "      <td>Category:Judiciary of Iran || Category:Judicia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EGN060702I0072</td>\n",
       "      <td>2</td>\n",
       "      <td>Wikipedia:Vital articles/Level/4 || Wikipedia:...</td>\n",
       "      <td>Wikipedia:Vital articles/Level/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EGN060702I0114</td>\n",
       "      <td>2</td>\n",
       "      <td>Category:February 1910 events || Category:Febr...</td>\n",
       "      <td>Category:February 1910 events || Category:Febr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0  1                                      column_values  \\\n",
       "0  EGN060702I0010  1  Marchamp || Saint-Maurice-de-Gourdans || Nivig...   \n",
       "1  EGN060702I0010  3  zone naturelle d'intérêt écologique, faunistiq...   \n",
       "2  EGN060702I0031  1  Category:Judiciary of Iran || Category:Judicia...   \n",
       "3  EGN060702I0072  2  Wikipedia:Vital articles/Level/4 || Wikipedia:...   \n",
       "4  EGN060702I0114  2  Category:February 1910 events || Category:Febr...   \n",
       "\n",
       "                                 clean_column_values  \n",
       "0  Marchamp || Saint-Maurice-de-Gourdans || Nivig...  \n",
       "1  zone naturelle d'intérêt écologique, faunistiq...  \n",
       "2  Category:Judiciary of Iran || Category:Judicia...  \n",
       "3                   Wikipedia:Vital articles/Level/4  \n",
       "4  Category:February 1910 events || Category:Febr...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_biodiv_cta_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>column_values</th>\n",
       "      <th>clean_column_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EGN060702I0010</td>\n",
       "      <td>1</td>\n",
       "      <td>Marchamp || Saint-Maurice-de-Gourdans || Nivig...</td>\n",
       "      <td>Marchamp || Saint-Maurice-de-Gourdans || Nivig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EGN060702I0010</td>\n",
       "      <td>3</td>\n",
       "      <td>zone naturelle d'intérêt écologique, faunistiq...</td>\n",
       "      <td>zone naturelle d'intérêt écologique, faunistiq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EGN060702I0031</td>\n",
       "      <td>1</td>\n",
       "      <td>Category:Judiciary of Iran || Category:Judicia...</td>\n",
       "      <td>Category:Judiciary of Iran || Category:Judicia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EGN060702I0072</td>\n",
       "      <td>2</td>\n",
       "      <td>Wikipedia:Vital articles/Level/4 || Wikipedia:...</td>\n",
       "      <td>Wikipedia:Vital articles/Level/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EGN060702I0114</td>\n",
       "      <td>2</td>\n",
       "      <td>Category:February 1910 events || Category:Febr...</td>\n",
       "      <td>Category:February 1910 events || Category:Febr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0  1                                      column_values  \\\n",
       "0  EGN060702I0010  1  Marchamp || Saint-Maurice-de-Gourdans || Nivig...   \n",
       "1  EGN060702I0010  3  zone naturelle d'intérêt écologique, faunistiq...   \n",
       "2  EGN060702I0031  1  Category:Judiciary of Iran || Category:Judicia...   \n",
       "3  EGN060702I0072  2  Wikipedia:Vital articles/Level/4 || Wikipedia:...   \n",
       "4  EGN060702I0114  2  Category:February 1910 events || Category:Febr...   \n",
       "\n",
       "                                 clean_column_values  \n",
       "0  Marchamp || Saint-Maurice-de-Gourdans || Nivig...  \n",
       "1  zone naturelle d'intérêt écologique, faunistiq...  \n",
       "2  Category:Judiciary of Iran || Category:Judicia...  \n",
       "3                   Wikipedia:Vital articles/Level/4  \n",
       "4  Category:February 1910 events || Category:Febr...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_biodiv_cta_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_biodiv_cta_target_0_1000 = table_biodiv_cta_target[0:1000]\n",
    "table_biodiv_cta_target_1000_2000 = table_biodiv_cta_target[1000:2000]\n",
    "table_biodiv_cta_target_2000_3000 = table_biodiv_cta_target[2000:3000]\n",
    "table_biodiv_cta_target_3000_4000 = table_biodiv_cta_target[3000:4000]\n",
    "table_biodiv_cta_target_4000_5000 = table_biodiv_cta_target[4000:5000]\n",
    "table_biodiv_cta_target_5000_5250 = table_biodiv_cta_target[5000:5250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local cache to store results\n",
    "wikidata_cache = {\"id_cache\": {}, \"related_cache\": {}}\n",
    "\n",
    "# Define rate limit decorator to allow 10 requests per second\n",
    "@sleep_and_retry\n",
    "@limits(calls=10, period=1)\n",
    "def rate_limited_request(url, params=None, headers=None):\n",
    "    return requests.get(url, params=params, headers=headers)\n",
    "\n",
    "def get_wikidata_id(label):\n",
    "    if \"id_cache\" not in wikidata_cache:\n",
    "        wikidata_cache[\"id_cache\"] = {}\n",
    "    if label in wikidata_cache[\"id_cache\"]:\n",
    "        return wikidata_cache[\"id_cache\"][label]\n",
    "\n",
    "    url = \"https://www.wikidata.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"wbsearchentities\",\n",
    "        \"format\": \"json\",\n",
    "        \"language\": \"en\",\n",
    "        \"search\": label.strip()\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = rate_limited_request(url, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        if data['search']:\n",
    "            label_id = data['search'][0]['id']\n",
    "            wikidata_cache[\"id_cache\"][label] = label_id\n",
    "            return label_id\n",
    "        else:\n",
    "            return None\n",
    "    except (requests.JSONDecodeError, KeyError) as e:\n",
    "        # print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_related_ids(label_id, property_id):\n",
    "    if \"related_cache\" not in wikidata_cache:\n",
    "        wikidata_cache[\"related_cache\"] = {}\n",
    "    cache_key = f\"{label_id}_{property_id}\"\n",
    "    if cache_key in wikidata_cache[\"related_cache\"]:\n",
    "        return wikidata_cache[\"related_cache\"][cache_key]\n",
    "\n",
    "    url = \"https://query.wikidata.org/sparql\"\n",
    "    query = f\"\"\"\n",
    "    SELECT ?related WHERE {{\n",
    "        wd:{label_id} wdt:{property_id} ?related.\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Accept\": \"application/sparql-results+json\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = rate_limited_request(url, params={'query': query}, headers=headers)\n",
    "        data = response.json()\n",
    "\n",
    "        related_ids = []\n",
    "        if data.get('results', {}).get('bindings'):\n",
    "            for binding in data['results']['bindings']:\n",
    "                related_uri = binding['related']['value']\n",
    "                related_id = related_uri.split('/')[-1]\n",
    "                related_ids.append(related_id)\n",
    "\n",
    "        wikidata_cache[\"related_cache\"][cache_key] = related_ids\n",
    "        return related_ids\n",
    "    except (requests.JSONDecodeError, KeyError) as e:\n",
    "        # print(f\"Error: {e}\")\n",
    "        return []\n",
    "\n",
    "def process_cell(cell_contents):\n",
    "    labels = [label.strip() for label in cell_contents.split('||')]\n",
    "    filtered_labels = [label for label in labels if label.lower() != 'nan']\n",
    "    unique_labels = list(set(filtered_labels))\n",
    "\n",
    "    subclass_dict = {}\n",
    "    for label in unique_labels:\n",
    "        label_id = get_wikidata_id(label)\n",
    "        if label_id:\n",
    "            instance_ids = get_related_ids(label_id, 'P31')\n",
    "            if instance_ids:\n",
    "                subclass_dict[label] = instance_ids\n",
    "            else:\n",
    "                subclass_ids = get_related_ids(label_id, 'P279')\n",
    "                subclass_dict[label] = subclass_ids if subclass_ids else [None]\n",
    "        else:\n",
    "            subclass_dict[label] = [None]\n",
    "\n",
    "    subclass_ids_list = []\n",
    "    for label in filtered_labels:\n",
    "        subclass_ids_list.extend(subclass_dict[label])\n",
    "\n",
    "    valid_subclass_ids = [subclass_id for subclass_id in subclass_ids_list if subclass_id is not None]\n",
    "\n",
    "    if valid_subclass_ids:\n",
    "        # Count occurrences of each ID\n",
    "        counts = Counter(valid_subclass_ids)\n",
    "        max_count = max(counts.values())\n",
    "        # Select IDs that have the maximum count\n",
    "        most_common_ids = [id for id, count in counts.items() if count == max_count]\n",
    "        return ', '.join(most_common_ids)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Save cache to file\n",
    "def save_cache():\n",
    "    with open('wikidata_cache.json', 'w') as cache_file:\n",
    "        json.dump(wikidata_cache, cache_file)\n",
    "\n",
    "# Load cache from file (when restarting script)\n",
    "def load_cache():\n",
    "    global wikidata_cache\n",
    "    try:\n",
    "        with open('wikidata_cache.json', 'r') as cache_file:\n",
    "            wikidata_cache = json.load(cache_file)\n",
    "            # Ensure cache structure is correct\n",
    "            if \"id_cache\" not in wikidata_cache:\n",
    "                wikidata_cache[\"id_cache\"] = {}\n",
    "            if \"related_cache\" not in wikidata_cache:\n",
    "                wikidata_cache[\"related_cache\"] = {}\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        wikidata_cache = {\"id_cache\": {}, \"related_cache\": {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# # Load the cache at the beginning of your notebook\n",
    "load_cache()\n",
    "\n",
    "# Now, let's process the DataFrame\n",
    "# Iterate over rows of the DataFrame\n",
    "for index, row in table_biodiv_cta_target_2000_3000.iterrows():\n",
    "    cell_contents = row['clean_column_values']\n",
    "    result = process_cell(cell_contents)\n",
    "    table_biodiv_cta_target_2000_3000.at[index, 'column_type_annotation_multi_max_1'] = result\n",
    "\n",
    "# # Save cache before script ends\n",
    "save_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_biodiv_cta_target_2000_3000.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_biodiv_cta_target_2000_3000.to_csv(\"table_biodiv_cta_target_2000_3000_r2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
